# blip-image-captioning
Implemented image captioning using the BLIP (Bootstrapped Language-Image Pretraining) model, which outperforms traditional CNN-LSTM architectures by leveraging vision-language pretraining for more accurate and context-aware captions, with significantly better evaluation metrics such as BLEU and CIDEr scores.
